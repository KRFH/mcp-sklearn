import os
import json
import asyncio
import pathlib

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_mcp_adapters.client import MultiServerMCPClient

# --- è¨­å®š ---
# å¿…è¦: OPENAI_API_KEY ã‚’ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
# ä¾‹) export OPENAI_API_KEY=sk-xxxx
OPENAI_MODEL = "gpt-4o-mini"  # é©å®œå¤‰æ›´å¯

# MCP ã‚µãƒ¼ãƒãƒ¼(= Docker)ã‚’ STDIO ã§éƒ½åº¦èµ·å‹•
# data ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã« -v ã‚’æŒ‡å®šï¼ˆçµ¶å¯¾ãƒ‘ã‚¹æ¨å¥¨ï¼‰
PROJECT_ROOT = pathlib.Path(__file__).resolve().parents[1]  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆæƒ³å®š: <repo>/
DATA_DIR = PROJECT_ROOT / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

DOCKER_ARGS = [
    "run",
    "--rm",
    "-i",
    "-v",
    f"{DATA_DIR}:/app/data",
    "krfh/mcp-sklearn:stdio",
]


async def main():
    st.set_page_config(page_title="OpenAI chat with MCP tools", page_icon="ğŸ§°")
    st.title("OpenAI chat with MCP tools (STDIO Docker)")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç®¡ç†ï¼ˆLangChainã®Messageå‹ã§ä¿æŒï¼‰
    if "messages" not in st.session_state:
        st.session_state.messages = []
    messages = st.session_state.messages

    # ç”»é¢ã¸ã®å¾©å…ƒè¡¨ç¤ºï¼ˆhuman/aiã®ã¿ï¼‰
    for m in messages:
        role = getattr(m, "type", None) or getattr(m, "role", None)
        content = getattr(m, "content", "")
        if role in ("human", "ai"):
            with st.chat_message(role):
                if isinstance(content, str):
                    st.write(content)
                elif isinstance(content, list):
                    # OpenAIã¯textãƒ‘ãƒ¼ãƒ„ã®ã“ã¨ãŒã‚ã‚‹
                    for c in content:
                        if isinstance(c, dict) and c.get("type") == "text":
                            st.write(c.get("text", ""))

    # å…¥åŠ›å—ä»˜
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›â€¦"):
        with st.chat_message("human"):
            st.write(prompt)
        messages.append(HumanMessage(prompt))

        # OpenAIãƒ¢ãƒ‡ãƒ«
        chat_model = ChatOpenAI(model=OPENAI_MODEL)

        # MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆSTDIOçµŒç”±ã§ docker run -i â€¦ ã‚’éƒ½åº¦èµ·å‹•ï¼‰
        client = MultiServerMCPClient(
            {
                "sklearn": {  # ä»»æ„ã®ã‚µãƒ¼ãƒãƒ¼åã‚­ãƒ¼
                    "command": "docker",
                    "args": DOCKER_ARGS,
                    "transport": "stdio",
                },
            }
        )
        tools = await client.get_tools()

        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒç¶šãé™ã‚Šãƒ«ãƒ¼ãƒ—
        while True:
            # ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰ã—ã¦æ¨è«–
            ai_response = await chat_model.bind_tools(tools).ainvoke(messages)
            messages.append(ai_response)

            # ç”»é¢ã«è¡¨ç¤º
            with st.chat_message("ai"):
                if isinstance(ai_response.content, str):
                    st.write(ai_response.content)
                else:
                    for c in ai_response.content or []:
                        if isinstance(c, dict) and c.get("type") == "text":
                            st.write(c.get("text", ""))

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚Œã°å®Ÿè¡Œã—ã¦ä¼šè©±ã«è¿½åŠ 
            if getattr(ai_response, "tool_calls", None):
                for call in ai_response.tool_calls:
                    # name ã¯å¤§å°åŒºåˆ¥ãªã matching
                    selected = {t.name.lower(): t for t in tools}[call["name"].lower()]
                    tool_msg = await selected.ainvoke(call)  # LangChainã®ToolMessageãŒè¿”ã‚‹
                    messages.append(tool_msg)
                    # ãƒ„ãƒ¼ãƒ«ã®çµæœã‚’ç”»é¢ã«ã‚‚è»½ãè¡¨ç¤º
                    with st.chat_message("ai"):
                        st.write(
                            f"ğŸ› ï¸ `{selected.name}` å®Ÿè¡Œ: {tool_msg.content[:500]}{'...' if len(str(tool_msg.content))>500 else ''}"
                        )
            else:
                break


if __name__ == "__main__":
    asyncio.run(main())
